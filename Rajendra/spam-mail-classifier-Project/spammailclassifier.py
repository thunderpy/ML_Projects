# -*- coding: utf-8 -*-
"""SpamMailClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QWRTPHvqZJxgkZFSzndOtBNS5XTUpHK9
"""

# Spam Mail Classifier

# To download upload files
from google.colab import files

files.upload()

# Import Libraries

# Handling Data
import pandas as pd
import numpy as np

# Visialization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')

# For Text processing 
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

# ML
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# DL
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, BatchNormalization, Flatten, Dropout
from tensorflow.keras.models import Sequential

# Accuracy Metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Reading Data
df = pd.read_csv('spam_ham_dataset.csv')

# Removing Unnecessary column
df.drop('Unnamed: 0', axis=1, inplace=True)

# Changing column names
df.columns = ['label', 'text', 'class']

df.head()

df.shape

df.info()

# No NaN in the data
df.isna().sum()

# Barplot describes the count of the class labels
plt.figure(figsize = (12, 6))
sns.countplot(data = df, x = 'label');

# Viewing samples of the data

# Let's see few examples of the data
for i in df.iterrows():
    print("Class Label: {}\nMail: \n{}\n\n".format(i[1][0], i[1][1]))
    if i[0] == 6:
        break

# Commented out IPython magic to ensure Python compatibility.
# # Remove stopwords from the data
# %%time
# stop_words = set(stopwords.words('english'))
# 
# df['text'] = df['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x)
#                 if not word in stop_words]))

print(stop_words)

# df.sample(10)
df.head()

X = df.loc[:, 'text']
y = df.loc[:, 'class']

print(f"Shape of X: {X.shape}\nShape of y: {y.shape}")

# Split data into train and test in 80:20

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=11)

print(f"Train Data Shape: {X_train.shape}\nTest Data Shape: {X_test.shape}")

# Preprocess text to build the ML model

cVect = CountVectorizer()
cVect.fit   (X_train)

# Let's see the vocabulary that has extracted by the count vextorizer.

print('NO.of Tokens: ',len(cVect.vocabulary_.keys()))

print(cVect.vocabulary_)

# document term vector (dtv)
dtv = cVect.transform(X_train)

type(dtv)

dtv = dtv.toarray()

print(dtv)

dtv.shape

print(f"Number of Observations: {dtv.shape[0]}\nTokens/Features: {dtv.shape[1]}")

# Let's see an sample that has been preprocessed
dtv[1]

# Logistic Regression.

# Logistic Regression could help use predict whether the student passed or failed.
# Logistic regression predictions are discrete (only specific values or categories are allowed).
# We can also view probability scores underlying the modelâ€™s classifications.

# Hyperparameter Tuning

lr = LogisticRegression(verbose=1)

grid = {"C":[float(i) for i in range(1, 3)], "penalty":["l1", "l2"], "solver":['lbfgs', 'liblinear']}

logreg_cv = GridSearchCV(lr, grid, cv=4)
logreg_cv.fit(dtv, y_train)

print("Tuned Hpyerparameters :",logreg_cv.best_params_)
print("accuracy :",logreg_cv.best_score_)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# lr = LogisticRegression(solver='liblinear', penalty ='l2' , C = 1.0)
# lr.fit(dtv, y_train)

# Evaluate on the Test data

# Preprocess the test data
test_dtv = cVect.transform(X_test)
test_dtv = test_dtv.toarray()

print(f"Number of Observations: {test_dtv.shape[0]}\nTokens/Features: {test_dtv.shape[1]}")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# pred = lr.predict(test_dtv)

print('Accuracy: ', accuracy_score(y_test, pred) * 100)

# Classification Report of the classifier

# 0 - Not Spam / Ham
# 1 - Spam 
print(classification_report(y_test, pred))

# Confusion Matrix

cmat = confusion_matrix(y_test, pred)
plt.figure(figsize = (6, 6))

sns.heatmap(cmat, annot = True, cmap = 'Paired', cbar = False, fmt="d", xticklabels=['Not Spam', 'Spam'], yticklabels=['Not Spam', 'Spam']);

# Predict Class label for the unseen data i.e., Spam or Not Spam

# 'You won 1000$ prize money in lottery. Click here to avail'
def predict_class(lr):
    text = input('Enter Text(Subject of the mail): ')
    #print(text)
    text = [" ".join([ word for word in word_tokenize(text)  if not word in stop_words])]
    #print(text)
    t_dtv = cVect.transform(text).toarray()
    #print("=========================")
    #print(t_dtv)
    print('Predicted Class:', end = ' ')
    print('Spam' if lr.predict(t_dtv)[0] else 'Not Spam')
    #print(lr.predict(t_dtv))
    #print("+++++++++++")
    prob = lr.predict_proba(t_dtv)*100
    print("Prob: ", prob)
    print(f"Not Spam: {prob[0][0]}%\nSpam: {prob[0][1]}%")
    plt.figure(figsize=(12, 6))
    sns.barplot(x =['Not Spam', 'Spam'] , y = [prob[0][0], prob[0][1]])
    plt.xlabel('Class')
    plt.ylabel('Probalility')
    plt.show()

predict_class(lr)

